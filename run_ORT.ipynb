{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7537ed53-c1c3-45d1-8243-4de3d18f586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "import torch\n",
    "from coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnxruntime import quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "275716a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolon_path = \"./trained_yolov10n/weights/best.onnx\"\n",
    "nanodet_path = \"nanodet.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ec43aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"xrjb7ahPJgT610pkOEf2\")\n",
    "project = rf.workspace(\"mooncrater\").project(\"mooncrater\")\n",
    "dataset = project.version(3).download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ec0b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training examples: 1479\n",
      "Number of validation examples: 80\n",
      "Number of test examples: 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision\n",
    "\n",
    "# settings\n",
    "ANNOTATION_FILE_NAME = \"_annotations.coco.json\"\n",
    "TRAIN_DIRECTORY = os.path.join(dataset.location, \"train\")\n",
    "VAL_DIRECTORY = os.path.join(dataset.location, \"valid\")\n",
    "TEST_DIRECTORY = os.path.join(dataset.location, \"test\")\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_directory_path: str,\n",
    "        # onnx_model_path: str,\n",
    "        train: bool = True\n",
    "    ):\n",
    "        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        \n",
    "        return images, annotations\n",
    "\n",
    "\n",
    "TRAIN_DATASET = CocoDetection(\n",
    "    image_directory_path=TRAIN_DIRECTORY,\n",
    "    train=True)\n",
    "VAL_DATASET = CocoDetection(\n",
    "    image_directory_path=VAL_DIRECTORY,\n",
    "    train=False)\n",
    "TEST_DATASET = CocoDetection(\n",
    "    image_directory_path=TEST_DIRECTORY,\n",
    "    train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
    "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
    "print(\"Number of test examples:\", len(TEST_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed645bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea26a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f330a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizationDataReader(quantization.CalibrationDataReader):\n",
    "    def __init__(self, test_ds, batch_size, input_name):\n",
    "\n",
    "        self.test_ds = test_ds\n",
    "\n",
    "        self.input_name = input_name\n",
    "        self.datasize = len(self.test_ds)\n",
    "\n",
    "        self.enum_data = iter(self.test_ds)\n",
    "\n",
    "    def to_numpy(self, image):\n",
    "        image = np.array(image)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        # print(image)\n",
    "        image = np.transpose(image, (2, 0, 1))  # Change data layout from HWC to CHW\n",
    "        image = np.expand_dims(image, axis=0) \n",
    "\n",
    "        return image\n",
    "\n",
    "    def get_next(self):\n",
    "        batch = next(self.enum_data, None)\n",
    "        if batch is not None:\n",
    "          return {self.input_name: self.to_numpy(batch[0])}\n",
    "        else:\n",
    "          return None\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = iter(self.torch_dl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f36890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b477eb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=640x640>,\n",
       " {'image_id': 2,\n",
       "  'annotations': [{'id': 7,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [74, 238, 35, 43.5],\n",
       "    'area': 1522.5,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 8,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [489, 261, 33.5, 24.5],\n",
       "    'area': 820.75,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 9,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [3, 304, 49, 61.5],\n",
       "    'area': 3013.5,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 10,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [397, 324, 68.5, 50],\n",
       "    'area': 3425,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 11,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [114, 344, 48.5, 30.5],\n",
       "    'area': 1479.25,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 12,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [43, 346, 56.5, 52],\n",
       "    'area': 2938,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 13,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [119, 372, 41.5, 28],\n",
       "    'area': 1162,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 14,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [255, 325, 186, 123.5],\n",
       "    'area': 22971,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 15,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [78, 407, 79.5, 76],\n",
       "    'area': 6042,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 16,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [223, 419, 74.5, 51],\n",
       "    'area': 3799.5,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 17,\n",
       "    'image_id': 2,\n",
       "    'category_id': 2,\n",
       "    'bbox': [471, 407, 162.5, 79],\n",
       "    'area': 12837.5,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 18,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [23, 423, 59, 52],\n",
       "    'area': 3068,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 19,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [277, 448, 180.5, 138],\n",
       "    'area': 24909,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 20,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [503, 572, 55.5, 37],\n",
       "    'area': 2053.5,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0},\n",
       "   {'id': 21,\n",
       "    'image_id': 2,\n",
       "    'category_id': 3,\n",
       "    'bbox': [270, 595, 74, 44],\n",
       "    'area': 3256,\n",
       "    'segmentation': [],\n",
       "    'iscrowd': 0}]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATASET[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac30c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# onnx_model = rt.InferenceSession(nanodet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84812bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cc8916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def inf_on_image(input_pil_image, onnx_model):\n",
    "    \n",
    "    image = np.array(input_pil_image)\n",
    "\n",
    "    # Preprocess the images as required by your ONNX model\n",
    "    # This might involve resizing, normalization, adding a batch dimension, etc.\n",
    "    # The exact preprocessing steps will depend on your specific model\n",
    "\n",
    "    # Convert the images to a format that can be used as input to the ONNX model\n",
    "    # print(image)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    # print(image)\n",
    "    image = np.transpose(image, (2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Run the ONNX model\n",
    "    input_name = onnx_model.get_inputs()[0].name\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    predictions = onnx_model.run(None, {input_name: image})\n",
    "    end = (time.perf_counter() - start) * 1000\n",
    "\n",
    "    return predictions, end\n",
    "\n",
    "def draw_preds(input_pil_image, predictions, conf=0.70):\n",
    "    draw = ImageDraw.Draw(input_pil_image)\n",
    "\n",
    "    # Define a color for each label\n",
    "    colors = {0: \"green\", 1: \"blue\", 2: \"red\"}\n",
    "\n",
    "    for prediction in predictions[0][0]:\n",
    "\n",
    "        x1, y1, x2, y2, score, label = prediction\n",
    "        \n",
    "        #print(score, label)\n",
    "\n",
    "        if score > conf:\n",
    "            # Draw the bounding box on the image\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=colors[int(label)], width=2)\n",
    "\n",
    "    # Display the image\n",
    "    input_pil_image.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50d8e272",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inf_on_image() missing 1 required positional argument: 'onnx_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_img, _ \u001b[38;5;241m=\u001b[39m TEST_DATASET[\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43minf_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m draw_preds(test_img, preds, \u001b[38;5;241m0.25\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: inf_on_image() missing 1 required positional argument: 'onnx_model'"
     ]
    }
   ],
   "source": [
    "test_img, _ = TEST_DATASET[4]\n",
    "\n",
    "preds = inf_on_image(test_img)\n",
    "\n",
    "print(preds[0].shape)\n",
    "\n",
    "draw_preds(test_img, preds, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "871db86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'image_id': 0,\n",
       "  'category_id': 2,\n",
       "  'bbox': [100, 402, 110.5, 38.5],\n",
       "  'area': 4254.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 1,\n",
       "  'image_id': 1,\n",
       "  'category_id': 2,\n",
       "  'bbox': [41, 303, 100.5, 29.5],\n",
       "  'area': 2964.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 2,\n",
       "  'image_id': 1,\n",
       "  'category_id': 2,\n",
       "  'bbox': [510, 330, 123.5, 46],\n",
       "  'area': 5681,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 3,\n",
       "  'image_id': 1,\n",
       "  'category_id': 2,\n",
       "  'bbox': [342, 339, 65, 33.5],\n",
       "  'area': 2177.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 4,\n",
       "  'image_id': 1,\n",
       "  'category_id': 2,\n",
       "  'bbox': [55, 361, 91.5, 34.5],\n",
       "  'area': 3156.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 5,\n",
       "  'image_id': 1,\n",
       "  'category_id': 2,\n",
       "  'bbox': [291, 396, 262.5, 79],\n",
       "  'area': 20737.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 6,\n",
       "  'image_id': 1,\n",
       "  'category_id': 2,\n",
       "  'bbox': [4, 427, 218.5, 50.5],\n",
       "  'area': 11034.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 7,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [74, 238, 35, 43.5],\n",
       "  'area': 1522.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 8,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [489, 261, 33.5, 24.5],\n",
       "  'area': 820.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 9,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [3, 304, 49, 61.5],\n",
       "  'area': 3013.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 10,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [397, 324, 68.5, 50],\n",
       "  'area': 3425,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 11,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [114, 344, 48.5, 30.5],\n",
       "  'area': 1479.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 12,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [43, 346, 56.5, 52],\n",
       "  'area': 2938,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 13,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [119, 372, 41.5, 28],\n",
       "  'area': 1162,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 14,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [255, 325, 186, 123.5],\n",
       "  'area': 22971,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 15,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [78, 407, 79.5, 76],\n",
       "  'area': 6042,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 16,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [223, 419, 74.5, 51],\n",
       "  'area': 3799.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 17,\n",
       "  'image_id': 2,\n",
       "  'category_id': 2,\n",
       "  'bbox': [471, 407, 162.5, 79],\n",
       "  'area': 12837.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 18,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [23, 423, 59, 52],\n",
       "  'area': 3068,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 19,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [277, 448, 180.5, 138],\n",
       "  'area': 24909,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 20,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [503, 572, 55.5, 37],\n",
       "  'area': 2053.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 21,\n",
       "  'image_id': 2,\n",
       "  'category_id': 3,\n",
       "  'bbox': [270, 595, 74, 44],\n",
       "  'area': 3256,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 22,\n",
       "  'image_id': 3,\n",
       "  'category_id': 3,\n",
       "  'bbox': [337, 304, 19.5, 16],\n",
       "  'area': 312,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 23,\n",
       "  'image_id': 3,\n",
       "  'category_id': 3,\n",
       "  'bbox': [285, 309, 20.5, 20],\n",
       "  'area': 410,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 24,\n",
       "  'image_id': 3,\n",
       "  'category_id': 2,\n",
       "  'bbox': [200, 344, 108, 27.5],\n",
       "  'area': 2970,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 25,\n",
       "  'image_id': 3,\n",
       "  'category_id': 2,\n",
       "  'bbox': [17, 335, 183, 61.5],\n",
       "  'area': 11254.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 26,\n",
       "  'image_id': 4,\n",
       "  'category_id': 2,\n",
       "  'bbox': [21, 380, 316.5, 52.5],\n",
       "  'area': 16616.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 27,\n",
       "  'image_id': 4,\n",
       "  'category_id': 2,\n",
       "  'bbox': [330, 495, 128.5, 57.5],\n",
       "  'area': 7388.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 28,\n",
       "  'image_id': 5,\n",
       "  'category_id': 3,\n",
       "  'bbox': [0, 237, 574, 402],\n",
       "  'area': 230748,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 29,\n",
       "  'image_id': 5,\n",
       "  'category_id': 2,\n",
       "  'bbox': [553, 437, 79, 34.5],\n",
       "  'area': 2725.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 30,\n",
       "  'image_id': 5,\n",
       "  'category_id': 3,\n",
       "  'bbox': [554, 558, 26, 36.5],\n",
       "  'area': 949,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 31,\n",
       "  'image_id': 6,\n",
       "  'category_id': 2,\n",
       "  'bbox': [83, 171, 229.5, 24.5],\n",
       "  'area': 5622.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 32,\n",
       "  'image_id': 6,\n",
       "  'category_id': 3,\n",
       "  'bbox': [172, 211, 32, 25],\n",
       "  'area': 800,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 33,\n",
       "  'image_id': 6,\n",
       "  'category_id': 3,\n",
       "  'bbox': [498, 216, 78.5, 47],\n",
       "  'area': 3689.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 34,\n",
       "  'image_id': 6,\n",
       "  'category_id': 3,\n",
       "  'bbox': [182, 237, 24, 20],\n",
       "  'area': 480,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 35,\n",
       "  'image_id': 6,\n",
       "  'category_id': 3,\n",
       "  'bbox': [331, 246, 44.5, 32.5],\n",
       "  'area': 1446.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 36,\n",
       "  'image_id': 6,\n",
       "  'category_id': 3,\n",
       "  'bbox': [296, 304, 47.5, 41.5],\n",
       "  'area': 1971.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 37,\n",
       "  'image_id': 6,\n",
       "  'category_id': 2,\n",
       "  'bbox': [15, 312, 170, 41],\n",
       "  'area': 6970,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 38,\n",
       "  'image_id': 6,\n",
       "  'category_id': 3,\n",
       "  'bbox': [131, 462, 93, 70],\n",
       "  'area': 6510,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 39,\n",
       "  'image_id': 6,\n",
       "  'category_id': 3,\n",
       "  'bbox': [207, 563, 47.5, 45],\n",
       "  'area': 2137.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 40,\n",
       "  'image_id': 6,\n",
       "  'category_id': 3,\n",
       "  'bbox': [302, 127, 33, 22],\n",
       "  'area': 726,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 41,\n",
       "  'image_id': 7,\n",
       "  'category_id': 2,\n",
       "  'bbox': [515, 397, 57.5, 21],\n",
       "  'area': 1207.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 42,\n",
       "  'image_id': 7,\n",
       "  'category_id': 2,\n",
       "  'bbox': [556, 414, 60.5, 18],\n",
       "  'area': 1089,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 43,\n",
       "  'image_id': 7,\n",
       "  'category_id': 2,\n",
       "  'bbox': [410, 448, 52.5, 21.5],\n",
       "  'area': 1128.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 44,\n",
       "  'image_id': 7,\n",
       "  'category_id': 2,\n",
       "  'bbox': [338, 462, 51.5, 26.5],\n",
       "  'area': 1364.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 45,\n",
       "  'image_id': 7,\n",
       "  'category_id': 2,\n",
       "  'bbox': [342, 491, 74.5, 21],\n",
       "  'area': 1564.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 46,\n",
       "  'image_id': 7,\n",
       "  'category_id': 2,\n",
       "  'bbox': [166, 506, 100.5, 30.5],\n",
       "  'area': 3065.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 47,\n",
       "  'image_id': 7,\n",
       "  'category_id': 3,\n",
       "  'bbox': [417, 504, 222, 135],\n",
       "  'area': 29970,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 48,\n",
       "  'image_id': 7,\n",
       "  'category_id': 3,\n",
       "  'bbox': [227, 563, 24, 24],\n",
       "  'area': 576,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 49,\n",
       "  'image_id': 7,\n",
       "  'category_id': 3,\n",
       "  'bbox': [11, 568, 38.5, 31.5],\n",
       "  'area': 1212.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 50,\n",
       "  'image_id': 7,\n",
       "  'category_id': 3,\n",
       "  'bbox': [224, 595, 32, 22],\n",
       "  'area': 704,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 51,\n",
       "  'image_id': 7,\n",
       "  'category_id': 3,\n",
       "  'bbox': [9, 615, 41, 24],\n",
       "  'area': 984,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 52,\n",
       "  'image_id': 7,\n",
       "  'category_id': 3,\n",
       "  'bbox': [218, 619, 42.5, 20],\n",
       "  'area': 850,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 53,\n",
       "  'image_id': 8,\n",
       "  'category_id': 2,\n",
       "  'bbox': [227, 325, 114, 54],\n",
       "  'area': 6156,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 54,\n",
       "  'image_id': 8,\n",
       "  'category_id': 3,\n",
       "  'bbox': [332, 497, 50.5, 50],\n",
       "  'area': 2525,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 55,\n",
       "  'image_id': 8,\n",
       "  'category_id': 3,\n",
       "  'bbox': [510, 556, 37.5, 34],\n",
       "  'area': 1275,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 56,\n",
       "  'image_id': 9,\n",
       "  'category_id': 2,\n",
       "  'bbox': [0, 200, 201, 46],\n",
       "  'area': 9246,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 57,\n",
       "  'image_id': 9,\n",
       "  'category_id': 3,\n",
       "  'bbox': [398, 260, 53, 33],\n",
       "  'area': 1749,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 58,\n",
       "  'image_id': 9,\n",
       "  'category_id': 2,\n",
       "  'bbox': [245, 305, 131, 40.5],\n",
       "  'area': 5305.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 59,\n",
       "  'image_id': 9,\n",
       "  'category_id': 2,\n",
       "  'bbox': [405, 330, 54.5, 24],\n",
       "  'area': 1308,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 60,\n",
       "  'image_id': 10,\n",
       "  'category_id': 3,\n",
       "  'bbox': [471, 188, 91, 67],\n",
       "  'area': 6097,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 61,\n",
       "  'image_id': 10,\n",
       "  'category_id': 3,\n",
       "  'bbox': [187, 201, 220.5, 242.5],\n",
       "  'area': 53471.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 62,\n",
       "  'image_id': 10,\n",
       "  'category_id': 1,\n",
       "  'bbox': [420, 230, 219, 311.5],\n",
       "  'area': 68218.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 63,\n",
       "  'image_id': 10,\n",
       "  'category_id': 3,\n",
       "  'bbox': [341, 26, 58, 45.5],\n",
       "  'area': 2639,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 64,\n",
       "  'image_id': 11,\n",
       "  'category_id': 2,\n",
       "  'bbox': [6, 278, 62, 23],\n",
       "  'area': 1426,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 65,\n",
       "  'image_id': 11,\n",
       "  'category_id': 3,\n",
       "  'bbox': [31, 289, 117, 36.5],\n",
       "  'area': 4270.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 66,\n",
       "  'image_id': 12,\n",
       "  'category_id': 2,\n",
       "  'bbox': [235, 153, 339, 82],\n",
       "  'area': 27798,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 67,\n",
       "  'image_id': 12,\n",
       "  'category_id': 1,\n",
       "  'bbox': [287, 245, 71, 291.5],\n",
       "  'area': 20696.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 68,\n",
       "  'image_id': 12,\n",
       "  'category_id': 3,\n",
       "  'bbox': [541, 382, 33.5, 40.5],\n",
       "  'area': 1356.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 69,\n",
       "  'image_id': 13,\n",
       "  'category_id': 3,\n",
       "  'bbox': [301, 492, 30, 25],\n",
       "  'area': 750,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 70,\n",
       "  'image_id': 13,\n",
       "  'category_id': 3,\n",
       "  'bbox': [227, 520, 17, 24.5],\n",
       "  'area': 416.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 71,\n",
       "  'image_id': 13,\n",
       "  'category_id': 3,\n",
       "  'bbox': [211, 547, 60.5, 65.5],\n",
       "  'area': 3962.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 72,\n",
       "  'image_id': 13,\n",
       "  'category_id': 3,\n",
       "  'bbox': [47, 568, 179, 71],\n",
       "  'area': 12709,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 73,\n",
       "  'image_id': 13,\n",
       "  'category_id': 3,\n",
       "  'bbox': [389, 584, 93, 50],\n",
       "  'area': 4650,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 74,\n",
       "  'image_id': 13,\n",
       "  'category_id': 3,\n",
       "  'bbox': [488, 599, 45, 38.5],\n",
       "  'area': 1732.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 75,\n",
       "  'image_id': 14,\n",
       "  'category_id': 3,\n",
       "  'bbox': [118, 233, 32.5, 35],\n",
       "  'area': 1137.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 76,\n",
       "  'image_id': 14,\n",
       "  'category_id': 3,\n",
       "  'bbox': [576, 312, 31.5, 22.5],\n",
       "  'area': 708.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 77,\n",
       "  'image_id': 14,\n",
       "  'category_id': 3,\n",
       "  'bbox': [451, 316, 35, 39.5],\n",
       "  'area': 1382.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 78,\n",
       "  'image_id': 14,\n",
       "  'category_id': 3,\n",
       "  'bbox': [612, 329, 25.5, 40],\n",
       "  'area': 1020,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 79,\n",
       "  'image_id': 14,\n",
       "  'category_id': 3,\n",
       "  'bbox': [572, 439, 26.5, 25],\n",
       "  'area': 662.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 80,\n",
       "  'image_id': 14,\n",
       "  'category_id': 3,\n",
       "  'bbox': [545, 485, 27.5, 26],\n",
       "  'area': 715,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 81,\n",
       "  'image_id': 14,\n",
       "  'category_id': 3,\n",
       "  'bbox': [602, 504, 37, 47],\n",
       "  'area': 1739,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 82,\n",
       "  'image_id': 14,\n",
       "  'category_id': 3,\n",
       "  'bbox': [387, 551, 74.5, 50.5],\n",
       "  'area': 3762.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 83,\n",
       "  'image_id': 15,\n",
       "  'category_id': 3,\n",
       "  'bbox': [300, 374, 41.5, 15.5],\n",
       "  'area': 643.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 84,\n",
       "  'image_id': 15,\n",
       "  'category_id': 2,\n",
       "  'bbox': [50, 476, 133, 37.5],\n",
       "  'area': 4987.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 85,\n",
       "  'image_id': 15,\n",
       "  'category_id': 2,\n",
       "  'bbox': [83, 537, 136.5, 28.5],\n",
       "  'area': 3890.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 86,\n",
       "  'image_id': 16,\n",
       "  'category_id': 3,\n",
       "  'bbox': [453, 150, 50, 50.5],\n",
       "  'area': 2525,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 87,\n",
       "  'image_id': 16,\n",
       "  'category_id': 3,\n",
       "  'bbox': [504, 175, 72, 54],\n",
       "  'area': 3888,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 88,\n",
       "  'image_id': 16,\n",
       "  'category_id': 3,\n",
       "  'bbox': [47, 258, 377, 381],\n",
       "  'area': 143637,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 89,\n",
       "  'image_id': 17,\n",
       "  'category_id': 3,\n",
       "  'bbox': [550, 263, 40, 29],\n",
       "  'area': 1160,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 90,\n",
       "  'image_id': 17,\n",
       "  'category_id': 3,\n",
       "  'bbox': [281, 304, 78.5, 47],\n",
       "  'area': 3689.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 91,\n",
       "  'image_id': 17,\n",
       "  'category_id': 3,\n",
       "  'bbox': [59, 400, 32, 34],\n",
       "  'area': 1088,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 92,\n",
       "  'image_id': 17,\n",
       "  'category_id': 2,\n",
       "  'bbox': [145, 392, 173, 57],\n",
       "  'area': 9861,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 93,\n",
       "  'image_id': 17,\n",
       "  'category_id': 3,\n",
       "  'bbox': [233, 449, 46.5, 36.5],\n",
       "  'area': 1697.25,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 94,\n",
       "  'image_id': 18,\n",
       "  'category_id': 2,\n",
       "  'bbox': [342, 264, 297, 50],\n",
       "  'area': 14850,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 95,\n",
       "  'image_id': 18,\n",
       "  'category_id': 3,\n",
       "  'bbox': [10, 378, 109, 42.5],\n",
       "  'area': 4632.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 96,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [8, 301, 40, 32],\n",
       "  'area': 1280,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 97,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [425, 314, 34, 30.5],\n",
       "  'area': 1037,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 98,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [355, 381, 47, 49],\n",
       "  'area': 2303,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 99,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [527, 402, 91, 41.5],\n",
       "  'area': 3776.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 100,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [206, 387, 201, 113.5],\n",
       "  'area': 22813.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 101,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [467, 468, 68, 37],\n",
       "  'area': 2516,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 102,\n",
       "  'image_id': 19,\n",
       "  'category_id': 2,\n",
       "  'bbox': [417, 460, 163, 82],\n",
       "  'area': 13366,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 103,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [3, 471, 97, 74.5],\n",
       "  'area': 7226.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 104,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [179, 484, 60.5, 51.5],\n",
       "  'area': 3115.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 105,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [207, 505, 189.5, 134],\n",
       "  'area': 25393,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 106,\n",
       "  'image_id': 19,\n",
       "  'category_id': 3,\n",
       "  'bbox': [580, 556, 58.5, 59.5],\n",
       "  'area': 3480.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATASET.coco.dataset['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "675bed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def calculate_metrics(inference_func, session):\n",
    "    evaluator = CocoEvaluator(coco_gt=TEST_DATASET.coco, iou_types=[\"bbox\"])\n",
    "\n",
    "    print(\"Running evaluation...\")\n",
    "\n",
    "    total_model_time = 0.0\n",
    "\n",
    "    for idx, (input_pil_image, annotation) in enumerate(TEST_DATASET):\n",
    "        \n",
    "        image_id = annotation['image_id']\n",
    "\n",
    "        predictions, elapsed_time = inference_func(input_pil_image, session)\n",
    "        total_model_time += elapsed_time\n",
    "        coco_results = []\n",
    "        for prediction in predictions[0][0]:\n",
    "\n",
    "            # Extract the bounding box coordinates, confidence score, and class label\n",
    "            x1, y1, x2, y2, score, label = prediction\n",
    "            \n",
    "            # Convert the bounding box from (x1, y1, x2, y2) to (x, y, width, height)\n",
    "            box = [x1, y1, x2 - x1, y2 - y1]\n",
    "\n",
    "            print(score,label)\n",
    "\n",
    "            if score > 0.25:\n",
    "                # Append the result\n",
    "                coco_results.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": int(label)+1,\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": float(score),\n",
    "                })\n",
    "        \n",
    "        if len(coco_results) > 0:\n",
    "            evaluator.update(coco_results)\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"Elapsed time: {total_model_time} seconds\")\n",
    "    \n",
    "\n",
    "    evaluator.synchronize_between_processes()\n",
    "    evaluator.accumulate()\n",
    "    evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2033617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'onnx_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43minf_on_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 14\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(inference_func)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (input_pil_image, annotation) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(TEST_DATASET):\n\u001b[1;32m     12\u001b[0m     image_id \u001b[38;5;241m=\u001b[39m annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m     predictions, elapsed_time \u001b[38;5;241m=\u001b[39m inference_func(input_pil_image, \u001b[43monnx_model\u001b[49m)\n\u001b[1;32m     15\u001b[0m     total_model_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m elapsed_time\n\u001b[1;32m     16\u001b[0m     coco_results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'onnx_model' is not defined"
     ]
    }
   ],
   "source": [
    "calculate_metrics(inf_on_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c01d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, _ = TEST_DATASET[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3157cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_options = rt.SessionOptions()\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "# Specify DNNL as the execution provider\n",
    "# session = rt.InferenceSession(yolon_path, sess_options, providers=['DnnlExecutionProvider'])\n",
    "session = rt.InferenceSession(yolon_path, sess_options, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca019226",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = rt.InferenceSession(yolon_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79eb6c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.get_inputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c913249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model_time(session, num=1000):\n",
    "    total_time = 0.0\n",
    "    num_runs = 30\n",
    "    for i in range(0,num_runs):\n",
    "        _, elp_time = inf_on_image(test_img, session)\n",
    "        total_time += elp_time\n",
    "    avg_time_ms = total_time / num_runs\n",
    "    return avg_time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7dba3db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.02335736650275"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0478e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdr = QuantizationDataReader(TEST_DATASET, batch_size=1, input_name=session.get_inputs()[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0901d089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': array([[[[0.01176471, 0.01176471, 0.01176471, ..., 0.02745098,\n",
       "           0.02745098, 0.02745098],\n",
       "          [0.01176471, 0.01176471, 0.01176471, ..., 0.02745098,\n",
       "           0.02745098, 0.02745098],\n",
       "          [0.01176471, 0.01176471, 0.01176471, ..., 0.02745098,\n",
       "           0.02745098, 0.02745098],\n",
       "          ...,\n",
       "          [0.2627451 , 0.23921569, 0.21176471, ..., 0.12156863,\n",
       "           0.10588235, 0.08627451],\n",
       "          [0.24313726, 0.23137255, 0.21568628, ..., 0.12156863,\n",
       "           0.10196079, 0.08627451],\n",
       "          [0.21176471, 0.21176471, 0.21176471, ..., 0.12156863,\n",
       "           0.10196079, 0.08627451]],\n",
       " \n",
       "         [[0.03137255, 0.03137255, 0.03137255, ..., 0.04705882,\n",
       "           0.04705882, 0.04705882],\n",
       "          [0.03137255, 0.03137255, 0.03137255, ..., 0.04705882,\n",
       "           0.04705882, 0.04705882],\n",
       "          [0.03137255, 0.03137255, 0.03137255, ..., 0.04705882,\n",
       "           0.04705882, 0.04705882],\n",
       "          ...,\n",
       "          [0.3137255 , 0.2901961 , 0.2627451 , ..., 0.13333334,\n",
       "           0.12156863, 0.10196079],\n",
       "          [0.29411766, 0.28235295, 0.26666668, ..., 0.13333334,\n",
       "           0.11764706, 0.10196079],\n",
       "          [0.2627451 , 0.2627451 , 0.2627451 , ..., 0.13333334,\n",
       "           0.11764706, 0.10196079]],\n",
       " \n",
       "         [[0.00784314, 0.00784314, 0.00784314, ..., 0.02352941,\n",
       "           0.02352941, 0.02352941],\n",
       "          [0.00784314, 0.00784314, 0.00784314, ..., 0.02352941,\n",
       "           0.02352941, 0.02352941],\n",
       "          [0.00784314, 0.00784314, 0.00784314, ..., 0.02352941,\n",
       "           0.02352941, 0.02352941],\n",
       "          ...,\n",
       "          [0.2784314 , 0.25490198, 0.23529412, ..., 0.09803922,\n",
       "           0.06666667, 0.04313726],\n",
       "          [0.25490198, 0.24705882, 0.23921569, ..., 0.09803922,\n",
       "           0.0627451 , 0.04313726],\n",
       "          [0.22352941, 0.22352941, 0.23529412, ..., 0.09803922,\n",
       "           0.0627451 , 0.04313726]]]], dtype=float32)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdr.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e90ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolon_path_prep = \"yolon_prep.onnx\"\n",
    "quantization.shape_inference.quant_pre_process(yolon_path, yolon_path_prep, skip_symbolic_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ac1f3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "q_static_opts = {\"ActivationSymmetric\":False,\n",
    "                \"WeightSymmetric\":True}\n",
    "if torch.cuda.is_available():\n",
    "    q_static_opts = {\"ActivationSymmetric\":True,\n",
    "                    \"WeightSymmetric\":True}\n",
    "\n",
    "model_int8_path = 'yolon_int8.onnx'\n",
    "quantized_model = quantization.quantize_static(model_input=yolon_path,\n",
    "                                               model_output=model_int8_path,\n",
    "                                               calibration_data_reader=qdr,\n",
    "                                               extra_options=q_static_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4a73df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_model = rt.InferenceSession(model_int8_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76cc265e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.7144072663165"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_model_time(int8_model, num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a6ced00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[[  0.      ,   0.      ,  16.95529 ,  11.303527,   0.      ,\n",
       "             0.      ],\n",
       "          [  0.      ,   0.      ,  16.95529 ,  11.303527,   0.      ,\n",
       "             0.      ],\n",
       "          [  0.      ,   0.      ,  16.95529 ,  11.303527,   0.      ,\n",
       "             0.      ],\n",
       "          ...,\n",
       "          [113.03527 ,   0.      , 186.5082  ,  16.95529 ,   0.      ,\n",
       "             0.      ],\n",
       "          [113.03527 ,   0.      , 186.5082  ,  16.95529 ,   0.      ,\n",
       "             0.      ],\n",
       "          [113.03527 ,   0.      , 186.5082  ,  16.95529 ,   0.      ,\n",
       "             0.      ]]], dtype=float32)],\n",
       " 239.04588799996418)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_on_image(test_img,int8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de4896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
